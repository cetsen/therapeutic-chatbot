{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Engagement and Satisfaction in Online Mental Health Platform Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import log10, floor, ceil\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import joblib\n",
    "import sklearn.externals\n",
    "import joblib\n",
    "from profanity_check import predict, predict_prob\n",
    "import sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "# Display long column text\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RED/annotated/100_annotated_dialogues.csv\")\n",
    "df = df.rename(columns={'conversation id': 'conversation_id', 'post title': 'post_title', 'dialog turn': 'dialog_turn', 'emotion prediction': 'emotion_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Group data by conversation id and calculate count of each conversation id\\ndf_conv_len = df.groupby(\"conversation_id\").count()\\ndf_conv_len = df_conv_len.drop(columns=[\"subreddit\", \"post_title\", \"author\", \"text\", \"compound\", \"sentiment\", \"emotion_prediction\"])\\nprint(\"Number of conversations in subreddit: \", len(df_conv_len))\\n\\n# Separate conversation id\\'s with a single occurrence as monologues\\ndf_mono = df_conv_len[df_conv_len[\"dialog_turn\"] == 1]\\n#print(\"Number of conversations with a single turn in subreddit: \", len(df_mono))\\ndf_mono_ids = df_mono.reset_index()\\ndf_mono_ids = df_mono_ids[\"conversation_id\"]\\n\\n# Separate conversation id\\'s with multiple occurrences as dialogues\\ndf_dia = df_conv_len[df_conv_len[\"dialog_turn\"] > 2]\\nprint(\"Number of conversations longer than 2 turns in subreddit: \", len(df_dia))\\ndf_dia = df_dia.reset_index()\\ndf_dia = df_dia.drop(columns=[\\'dialog_turn\\'])\\n\\n# Join dialogue conversation id\\'s with original data such that only dialogues remain in the dataset\\ndf = df.join(df_dia.set_index(\\'conversation_id\\'), on=\\'conversation_id\\', how=\"right\") \\n\\n# Separate conversations that have more than one author\\ndf_conv_authors = df.groupby(\"conversation_id\")[\"author\"].unique().reset_index()\\ndf_conv_authors[\"author\"] = df_conv_authors[\"author\"].apply(lambda x: x.size)\\ndf_conv_authors = df_conv_authors[df_conv_authors[\"author\"] > 1]\\ndf_conv_authors = df_conv_authors.drop(columns=[\\'author\\'])\\n\\n# Join dialogue conversation id\\'s with original data such that only conversations that have more than one author remain in the dataset\\ndf = df.join(df_conv_authors.set_index(\\'conversation_id\\'), on=\\'conversation_id\\', how=\"right\") \\nprint(\"Number of conversations longer than 2 turns with more than a single author in subreddit: \", len(df_conv_authors))\\n\\n### ---------------------------------------------- ###\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### --- CLEANING OUT CONVERSATIONS WITH ONLY 1 OR 2 TURN(S) AND/OR WITH A SINGLE AUTHOR --- ###\n",
    "'''\n",
    "# Group data by conversation id and calculate count of each conversation id\n",
    "df_conv_len = df.groupby(\"conversation_id\").count()\n",
    "df_conv_len = df_conv_len.drop(columns=[\"subreddit\", \"post_title\", \"author\", \"text\", \"compound\", \"sentiment\", \"emotion_prediction\"])\n",
    "print(\"Number of conversations in subreddit: \", len(df_conv_len))\n",
    "\n",
    "# Separate conversation id's with a single occurrence as monologues\n",
    "df_mono = df_conv_len[df_conv_len[\"dialog_turn\"] == 1]\n",
    "#print(\"Number of conversations with a single turn in subreddit: \", len(df_mono))\n",
    "df_mono_ids = df_mono.reset_index()\n",
    "df_mono_ids = df_mono_ids[\"conversation_id\"]\n",
    "\n",
    "# Separate conversation id's with multiple occurrences as dialogues\n",
    "df_dia = df_conv_len[df_conv_len[\"dialog_turn\"] > 2]\n",
    "print(\"Number of conversations longer than 2 turns in subreddit: \", len(df_dia))\n",
    "df_dia = df_dia.reset_index()\n",
    "df_dia = df_dia.drop(columns=['dialog_turn'])\n",
    "\n",
    "# Join dialogue conversation id's with original data such that only dialogues remain in the dataset\n",
    "df = df.join(df_dia.set_index('conversation_id'), on='conversation_id', how=\"right\") \n",
    "\n",
    "# Separate conversations that have more than one author\n",
    "df_conv_authors = df.groupby(\"conversation_id\")[\"author\"].unique().reset_index()\n",
    "df_conv_authors[\"author\"] = df_conv_authors[\"author\"].apply(lambda x: x.size)\n",
    "df_conv_authors = df_conv_authors[df_conv_authors[\"author\"] > 1]\n",
    "df_conv_authors = df_conv_authors.drop(columns=['author'])\n",
    "\n",
    "# Join dialogue conversation id's with original data such that only conversations that have more than one author remain in the dataset\n",
    "df = df.join(df_conv_authors.set_index('conversation_id'), on='conversation_id', how=\"right\") \n",
    "print(\"Number of conversations longer than 2 turns with more than a single author in subreddit: \", len(df_conv_authors))\n",
    "\n",
    "### ---------------------------------------------- ###\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numbers to a given number of significant figures (default = 2)\n",
    "def round_sig(x, sig=2):\n",
    "    if x != 0:\n",
    "        return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting conversation, speaker, and listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_responses(conversation_id,subreddit):\n",
    "    conversation = df[df[\"conversation_id\"] == conversation_id]\n",
    "    conversation = conversation[conversation[\"subreddit\"] == subreddit]\n",
    "    conversation.reset_index(drop=True, inplace=True)\n",
    "    speaker = conversation.author.iloc[0]\n",
    "    listener = conversation[conversation[\"author\"] != speaker][\"author\"].unique().item() \n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    listener_responses = conversation[conversation[\"author\"] == listener]\n",
    "    num_speaker_responses = len(speaker_responses) \n",
    "    num_listener_responses = len(listener_responses)\n",
    "    # TOIMPROVE: you can only keep conversation, speaker, and listener (because others are dynamic and can be calculated from these 3)\n",
    "    return conversation, speaker, listener, speaker_responses, listener_responses, num_speaker_responses, num_listener_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the level of engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates token length for all speaker responses except for the first and adds it to a new column 'token_length', and returns their sum.\n",
    "def calculate_speaker_token_length(conversation, speaker):\n",
    "    # Create an empty dataframe column 'token_length'\n",
    "    conversation.loc[:,'token_length'] = np.nan\n",
    "    \n",
    "    sum_token_length = 0\n",
    "    \n",
    "    for i in range(1, len(conversation)):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            # Tokenize speaker response and filter punctuations\n",
    "            tokens = nltk.word_tokenize(conversation[\"text\"][i])\n",
    "            tokens = list(filter(lambda tokens: tokens not in string.punctuation, tokens)) # TOIMPROVE: exclude quotation marks\n",
    "            \n",
    "            # Add token length to dataframe\n",
    "            conversation.at[i,'token_length'] = len(tokens)\n",
    "            \n",
    "            # Calculate sum of all tokens by speaker\n",
    "            sum_token_length += len(tokens)\n",
    "        \n",
    "    return conversation, sum_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_speaker_responses(conversation, speaker, listener):\n",
    "    i = 0\n",
    "    \n",
    "    while i < (len(conversation) - 1):\n",
    "        if conversation['author'].iloc[i] == speaker and conversation['author'].iloc[i+1] == speaker:\n",
    "            if (conversation['dialog_turn'].iloc[i] + 1) == conversation['dialog_turn'].iloc[i+1]:\n",
    "                dropped_turn = conversation['dialog_turn'].iloc[i+1]\n",
    "                former_text = conversation['text'][i]\n",
    "                latter_text = conversation['text'][i+1]\n",
    "\n",
    "                # Merge consecutive responses of the speaker\n",
    "                merged_text = former_text + \" \" + latter_text \n",
    "                conversation['text'].replace({former_text: merged_text}, inplace=True)\n",
    "\n",
    "                # Get names of indexes for which column dialog_turn has value of the dropped turn\n",
    "                conversation_index_names = conversation[conversation['dialog_turn'] == dropped_turn].index\n",
    "\n",
    "                # Delete these row indexes from dataframe\n",
    "                conversation.drop(conversation_index_names, inplace=True)\n",
    "\n",
    "                # Reset indexes\n",
    "                conversation.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "        i += 1\n",
    "        \n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    listener_responses = conversation[conversation[\"author\"] == listener]\n",
    "    num_speaker_responses = len(speaker_responses) \n",
    "    num_listener_responses = len(listener_responses)\n",
    "                 \n",
    "    return conversation, num_speaker_responses, num_listener_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if conversation is interleaved. \n",
    "# If all the even-numbered turns are by the speaker and if all the odd-numbered turns are by the listener, returns True. Otherwise, returns False.\n",
    "def is_interleaved_conversation(speaker, listener, conversation):\n",
    "    num_turns = len(conversation)\n",
    "    \n",
    "    for i in range(0, num_turns, 2):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    for i in range(1, num_turns, 2):\n",
    "        if conversation['author'].iloc[i] == listener:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_engagement_score(num_turns, interleaved, sum_token_length, diff):\n",
    "    num_turns_weight = 1\n",
    "    interleaved_weight = 1\n",
    "    token_length_weight = 0.05\n",
    "    diff_weight = -1\n",
    "\n",
    "    # Set an upper limit to token length's effect on engagement\n",
    "    if sum_token_length >= 30:\n",
    "        token_length_score = 30\n",
    "    else:\n",
    "        token_length_score = sum_token_length\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    if interleaved == True:\n",
    "        interleaved_int = 1\n",
    "    else:\n",
    "        interleaved_int = -1\n",
    "    \n",
    "    return num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_engagement(speaker, listener, conversation):\n",
    "    conversation, num_speaker_responses, num_listener_responses = merge_consecutive_speaker_responses(conversation, speaker, listener)\n",
    "    conversation, sum_token_length = calculate_speaker_token_length(conversation, speaker)\n",
    "    diff = num_speaker_responses - num_listener_responses\n",
    "    diff = abs(diff)\n",
    "    interleaved = is_interleaved_conversation(speaker, listener, conversation)\n",
    "    num_turns = len(conversation)\n",
    "    engagement = \"\"\n",
    "    threshold = 4\n",
    "    \n",
    "    engagement_score = calculate_engagement_score(num_turns, interleaved, sum_token_length, diff)\n",
    "    \n",
    "    if engagement_score >= threshold:\n",
    "        engagement = 1\n",
    "    else:\n",
    "        engagement = 0\n",
    "                    \n",
    "    return num_turns, interleaved, sum_token_length, round_sig(diff), round_sig(engagement_score), engagement, conversation, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the level of satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_sentiment(conversation, speaker):   \n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    sns.set_theme(style=\"white\")\n",
    "    g = sns.relplot(x=\"dialog_turn\", y=\"strongest_compound\", hue=\"sentiment\", style=\"emotion_prediction\", palette=\"Set1\",data=speaker_responses, s=200)\n",
    "    g.fig.suptitle('Sentimental and Emotional Shift in Speaker Responses with Conversation Progression', fontsize=16)\n",
    "    g.fig.subplots_adjust(top=0.9);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all emotions and the final emotion of the given responses\n",
    "def get_emotion_prediction(speaker_responses):\n",
    "    emotions = speaker_responses[\"emotion_prediction\"]\n",
    "    final_emotion = emotions.iloc[-1]\n",
    "    \n",
    "    return emotions, final_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all sentiments and the final sentiment of the given responses\n",
    "def get_sentiment(speaker_responses):\n",
    "    sentiments = speaker_responses[\"sentiment\"]\n",
    "    final_sentiment = sentiments.iloc[-1]\n",
    "    \n",
    "    return sentiments, final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the last speaker emotion is grateful and its sentiment is positive. If yes, return true. Otherwise, return false.\n",
    "def is_tagged_grateful_positive(speaker_responses):\n",
    "    _, final_sentiment = get_sentiment(speaker_responses)\n",
    "    _, final_emotion = get_emotion_prediction(speaker_responses)\n",
    "    \n",
    "    if final_sentiment == \"positive\" and final_emotion == \"grateful\":\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_toward_listener(speaker_response):    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    phrases = ['you', 'your']\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('toward_listener', None, *patterns)\n",
    "    sentence = nlp (speaker_response)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    " \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain profanity towards the listener\n",
    "def contains_profanity(conversation, speaker):\n",
    "    for i in range(1,len(conversation)):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            for j in range(0,len(conversation['sentences'].iloc[i])):\n",
    "                # i'th dialogue turn, j'th sentence\n",
    "                if predict([conversation['sentences'][i][j]]) == 1 and is_toward_listener(conversation['sentences'][i][j]) == True:\n",
    "                    # uncomment to print the sentence that contains profanity\n",
    "                    #print(conversation['sentences'][i][j])\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_gratitude(conversation, speaker):\n",
    " \n",
    "    # Take the speaker responses except first one\n",
    "    speaker_responses = conversation[conversation['author'] == speaker]\n",
    "    speaker_responses = speaker_responses[speaker_responses['dialog_turn'] != 1]\n",
    "    speaker_responses = speaker_responses['text']\n",
    "    speaker_responses = speaker_responses.to_string()[1:].lower()\n",
    "    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "   \n",
    "    phrases = ['thank', 'means a lot to me', 'thanks', 'appreciate', 'support', 'concern'\n",
    "               'your help', 'means so much to me', 'grateful', 'kind of you', 'repay you', \n",
    "               'taking the time']\n",
    "\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('gratitude', None, *patterns)\n",
    "    sentence = nlp (speaker_responses)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    "    \n",
    "    # uncomment this part if you want to print the matched phrases\n",
    "    #for match_id, start, end in matched_phrases:\n",
    "        #string_id = nlp.vocab.strings[match_id]  \n",
    "        #span = sentence[start:end]                   \n",
    "        #print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain sarcasm\n",
    "def contains_sarcasm(conversation, speaker, tokenizer, model):\n",
    "\n",
    "    # https://www.linkedin.com/pulse/you-being-sarcastic-deep-learning-answers-code-ibrahim-sobh-phd/?articleId=6662861432498987008\n",
    "    \n",
    "    # Take the speaker responses except first one\\\n",
    "    speaker_sentences = conversation[conversation['author'] == speaker]\n",
    "    speaker_sentences = speaker_sentences[speaker_sentences['dialog_turn'] != 1]\n",
    "    speaker_sentences = speaker_sentences['sentences']\n",
    "    sarcastic_probas = sarcastic.proba(speaker_sentences, tokenizer, model)\n",
    "    #print(sarcastic_probas)\n",
    "    \n",
    "    # Can be optimized \n",
    "    if (sarcastic_probas > 0.6).any():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain disagreement\n",
    "def contains_disagreement(conversation, speaker):\n",
    "        \n",
    "    # Take the speaker responses except first one\n",
    "    speaker_responses = conversation[conversation['author'] == speaker]\n",
    "    speaker_responses = speaker_responses[speaker_responses['dialog_turn'] != 1]\n",
    "    speaker_responses = speaker_responses['text']\n",
    "    speaker_responses = speaker_responses.to_string()[1:].lower()\n",
    "    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "   \n",
    "    phrases = [\"i don't think so\", \"no way\", \"disagree\", \"i beg to differ\", \"i'd say the exact opposite\", \n",
    "               \"not necessarily\", \"that's not always true\", \"that's not always the case\", \"i'm not so sure about that\", \n",
    "               \"that doesn’t make much sense to me\", \"i don’t share your view\", \"i don’t agree with you\"]\n",
    "\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('disagreement', None, *patterns)\n",
    "    sentence = nlp (speaker_responses)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    "    \n",
    "    # uncomment this part if you want to print the matched phrases\n",
    "    #for match_id, start, end in matched_phrases:\n",
    "        #string_id = nlp.vocab.strings[match_id]  \n",
    "        #span = sentence[start:end]                   \n",
    "        #print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column with sentence-level sentiment compounds\n",
    "def sentence_level_sentiment(conversation):\n",
    "    conversation['sentences'] = conversation['text'].apply(lambda x: sent_tokenize(x))\n",
    "    conversation['sentences'] = conversation['sentences'].map(lambda x: list(map(str.lower, x)))\n",
    "    conversation['sentence_compounds'] = conversation['sentences']\n",
    "    \n",
    "    for i in range(0,len(conversation)):\n",
    "        num_sentences = len(sent_tokenize(conversation['text'].iloc[i]))\n",
    "        # sentiment compound for each sentence\n",
    "        scores = np.zeros(num_sentences) \n",
    "        for j in range(0,num_sentences):\n",
    "            # i'th dialogue turn, j'th sentence\n",
    "            scores[j] = sid.polarity_scores(sent_tokenize(conversation['text'][i])[j])['compound']\n",
    "\n",
    "            conversation['sentence_compounds'][i] = scores\n",
    "            \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column with the sentence compound with strongest magnitude within a dialogue turn\n",
    "def strongest_sentiment(conversation):\n",
    "    conversation['strongest_compound'] = conversation['sentence_compounds']\n",
    "    conversation['strongest_compound'] = conversation['strongest_compound'].apply(lambda x: np.min(x) if np.max(abs(x)) == abs(np.min(x)) else np.max(x))\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IN LATER WEEKS?\n",
    "def emobert_predict_sentence_emotion(conversation):\n",
    "    conversation['sentence_level_emotion_prediction'] = conversation['sentences']\n",
    "    \n",
    "    for i in range(0,len(conversation)):\n",
    "        num_sentences = len(sent_tokenize(conversation['text'].iloc[i]))\n",
    "        # emotion for each sentence\n",
    "        emotions = np.zeros(num_sentences) \n",
    "        for j in range(0,num_sentences):\n",
    "            # i'th dialogue turn, j'th sentence\n",
    "            emotions[j] = sent_tokenize(conversation['text'][i])[j].swifter.apply(emobert_predict_emotion)\n",
    "            conversation['sentence_level_emotion_prediction'][i] = emotions\n",
    "            \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_satisfaction_score(slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty):\n",
    "    slope_weight = 1\n",
    "    sentiment_change_weight = 1\n",
    "    grateful_bonus_weight = 1.5\n",
    "    profanity_penalty_weight = 1\n",
    "    sarcasm_penalty_weight = 1\n",
    "    disagreement_penalty_weight = 1.5\n",
    "    threshold = 2\n",
    "    \n",
    "    return (slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + \n",
    "            profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_satisfaction(conversation, speaker, tokenizer, model):\n",
    "    conversation = sentence_level_sentiment(conversation)\n",
    "    conversation = strongest_sentiment(conversation)\n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    num_speaker_responses = len(speaker_responses)\n",
    "    \n",
    "    satisfaction = \"\"\n",
    "    grateful_bonus = 0\n",
    "    profanity_penalty = 0\n",
    "    sarcasm_penalty = 0\n",
    "    disagreement_penalty = 0\n",
    "    \n",
    "    # Change in sentiment from the first to the last turn\n",
    "    sentiment_change = speaker_responses['strongest_compound'].iloc[-1] - speaker_responses['strongest_compound'].iloc[0]\n",
    "    \n",
    "    # Take the slope of the compounds of speaker responses\n",
    "    f = np.polyfit(speaker_responses['dialog_turn'], speaker_responses['compound'], deg=1)\n",
    "    slope = f[0]\n",
    "\n",
    "    if is_tagged_grateful_positive(speaker_responses)==True or contains_gratitude(speaker_responses, speaker)==True:\n",
    "        grateful_bonus = 1\n",
    "        \n",
    "    if contains_profanity(conversation, speaker) == True:\n",
    "        profanity_penalty = -1\n",
    "    \n",
    "    if contains_sarcasm(conversation, speaker, tokenizer, model) == True:\n",
    "        sarcasm_penalty = -1    \n",
    "    \n",
    "    if contains_disagreement(conversation, speaker) == True:\n",
    "        disagreement_penalty = -1\n",
    "         \n",
    "    satisfaction_score, threshold = math.ceil(calculate_satisfaction_score(slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty))\n",
    "                                         \n",
    "    if satisfaction_score >= threshold:\n",
    "        satisfaction = 1\n",
    "    else:\n",
    "        satisfaction = 0\n",
    "        \n",
    "    return round_sig(slope), round_sig(sentiment_change), grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty, satisfaction_score, satisfaction, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the measures on a few dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sarcastic model once\n",
    "tokenizer, model = sarcastic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_examples(conversation_id):\n",
    "    conversation, speaker, listener, speaker_responses, listener_responses, num_speaker_responses, num_listener_responses = extract_responses(conversation_id)\n",
    "    turns, interleaved, num_speak_tok, diff, eng_score, eng, conversation, thres_e = classify_engagement(speaker, listener, conversation)\n",
    "    slope, change, grateful, profanity, sarcasm, disagree, sat_score, sat, thres_s = classify_satisfaction(conversation, speaker, tokenizer, model)\n",
    "    \n",
    "    plot_emotion_sentiment(conversation, speaker)\n",
    "    \n",
    "    print(\"Number of turns: \", turns, \", interleaved: \", interleaved, \", number of speaker tokens: \",  num_speak_tok, \n",
    "          \", speaker-listener turn diff: \",  diff, \", threshold: \", thres_e, \", ENGAGEMENT SCORE: \",  eng_score, \", ENGAGEMENT: \",  eng)\n",
    "    \n",
    "    print(\"Sentiment slope: \", slope, \", sentiment change: \", change, \", grateful bonus: \", grateful, \n",
    "          \", profanity penalty: \", profanity, \", sarcasm penalty: \", sarcasm, \", disagreement penalty: \", disagree, \n",
    "          \", threshold: \", thres_s, \", SATISFACTION SCORE: \", sat_score, \", SATISFACTION : \", sat) \n",
    "    \n",
    "    print(\"Classification summary: \", eng, \", \", sat)\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly engaging, highly satisfying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_examples(1732) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly engaging, highly satisfying (profanity & gratitude):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_examples(1003) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly engaging, less satisfying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_examples(854) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less engaging, highly satisfying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_examples(35) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly engaging, less satisfying (sarcasm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_examples(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the randomly selected 100 dialogues from 8 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(TP,FP,TN,FN):\n",
    "    P = TP/(TP+FP)\n",
    "    R = TP/(TP+FN)\n",
    "    f1 = 2*P*R/(P+R)\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "        \n",
    "    return P, R, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_satisfaction = {'slope_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sentiment_change_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'grateful_bonus_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'], \n",
    "                          'profanity_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sarcasm_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'disagreement_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sat_threshold': ['1.25', '1.5', '1.75', '2.0', '2.25']}\n",
    "\n",
    "param_grid_engagement = {'num_turns_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                        'interleaved_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                        'token_length_weight': ['0.02', '0.04', '0.06', '0.08', '0.10'],\n",
    "                        'diff_weight': ['-0.2', '-0.4', '-0.6', '-0.8', '-1.0'],\n",
    "                        'eng_threshold': ['3.25', '3.5', '3.75', '4.0', '4.25']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['conversation_id','subreddit']).groups\n",
    "\n",
    "def test(grouped): \n",
    "    TP_s = 0\n",
    "    TN_s = 0\n",
    "    FP_s = 0\n",
    "    FN_s = 0\n",
    "\n",
    "    TP_e = 0\n",
    "    TN_e = 0\n",
    "    FP_e = 0\n",
    "    FN_e = 0\n",
    "\n",
    "    for conv_id,subreddit in grouped:\n",
    "        conversation, speaker, listener, speaker_responses, listener_responses, num_speaker_responses, num_listener_responses = extract_responses(conv_id,subreddit)\n",
    "        turns, interleaved, num_speak_tok, diff, eng_score, eng, conversation, thres_e = classify_engagement(speaker, listener, conversation)\n",
    "        slope, change, grateful, profanity, sarcasm, disagree, sat_score, sat, thres_s = classify_satisfaction(conversation, speaker, tokenizer, model)\n",
    "\n",
    "        ground_sat = conversation['ground_truth_satisfaction']\n",
    "        if ((ground_sat == 1) & (sat == 1)).all():\n",
    "            TP_s += 1\n",
    "        elif ((ground_sat == 0) & (sat == 1)).all():\n",
    "            FP_s += 1\n",
    "        elif ((ground_sat == 1) & (sat == 0)).all():\n",
    "            FN_s += 1\n",
    "        else:\n",
    "            TN_s += 1\n",
    "\n",
    "        ground_eng = conversation['ground_truth_engagement']    \n",
    "        if ((ground_eng == 1) & (eng == 1)).all():\n",
    "            TP_e += 1\n",
    "        elif ((ground_eng == 0) & (eng == 1)).all():\n",
    "            FP_e += 1\n",
    "        elif ((ground_eng == 1) & (eng == 0)).all():\n",
    "            FN_e += 1\n",
    "        else:\n",
    "            TN_e += 1\n",
    "\n",
    "    P_s, R_s, f1_s, acc_s = performance(TP_s, FP_s, TN_s, FN_s)\n",
    "    P_e, R_e, f1_e, acc_e = performance(TP_e, FP_e, TN_e, FN_e)\n",
    "    \n",
    "    print(round_sig(P_s), round_sig(R_s), round_sig(f1_s), round_sig(acc_s))\n",
    "    print(round_sig(P_e), round_sig(R_e), round_sig(f1_e), round_sig(acc_e))\n",
    "    \n",
    "    return P_s, R_s, f1_s, acc_s, P_e, R_e, f1_e, acc_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tested separately on satisfaction and engagement to be able to tune correctly and more easily. Later to be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note / ask:\n",
    "1. I had trouble loading the larger files into memory which took a lot of time. There are ~60 annotated dialogues currently, but I managed to solve the issue and the rest will come.\n",
    "2. I couldn't apply cross-validation yet because the code needs to be modified such that I can run the measures on all dataframe (currently running by selecting each conversation_id from the dataframe one-by-one). \n",
    "3. The test function needs to be modified as well because now I will need to predict for all conversations at once and then test their performance. Whereas right now, I predict and test each sample at once.\n",
    "3. Depression_help conversation_id 9610 is in Portuguese. Should we remove foreign languages?\n",
    "4. Suicidewatch contains posts that are spam (repeated 100's of times) and they are missing their last few columns. Should we remove spam posts? (Some of them are mean)\n",
    "5. There are conversations with incorrect dialogue turns, e.g. 1, 2, 1, 2, 1, 2, 3, 4, 5,... Should we correct these?\n",
    "6. \"Dyadic\" conversation files contain multi conversations. Should we remove such dialogues?\n",
    "7. There are conversations with only 1 speaker turn. I should remove those.\n",
    "8. I have been just using dyadic conversations. Should I include multi as well? The algorithms would need to change. \n",
    "9. What would be the best way to combine the satisfaction and engagement performances? Because if even one of the engagement / satisfaction measures are low, the conversation is disqualified. Should we judge the classifier based on whether it disqualifies a conversation (predicts if one of the measures are low) or whether it predicts both measures correctly? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
