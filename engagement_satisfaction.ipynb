{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Engagement and Satisfaction in Online Mental Health Platform Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LinearSVC from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import log10, floor, ceil\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import joblib\n",
    "from profanity_check import predict, predict_prob\n",
    "import sarcastic\n",
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "# Display long column text\n",
    "pd.options.display.max_colwidth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RED/annotated/100_annotated_dialogues.csv\")\n",
    "df = df.rename(columns={'conversation id': 'conversation_id', 'post title': 'post_title', 'dialog turn': 'dialog_turn', 'emotion prediction': 'emotion_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numbers to a given number of significant figures (default = 2)\n",
    "def round_sig(x, sig=2):\n",
    "    if x != 0:\n",
    "        return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting conversation, speaker, and listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_responses(conversation_id,subreddit):\n",
    "    conversation = df[df[\"conversation_id\"] == conversation_id]\n",
    "    conversation = conversation[conversation[\"subreddit\"] == subreddit]\n",
    "    conversation.reset_index(drop=True, inplace=True)\n",
    "    speaker = conversation.author.iloc[0]\n",
    "    listener = conversation[conversation[\"author\"] != speaker][\"author\"].unique().item() \n",
    "\n",
    "    return conversation, speaker, listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates token length for all speaker responses except for the first and adds it to a new column 'token_length', and returns their sum.\n",
    "def calculate_speaker_token_length(conversation, speaker):\n",
    "    # Create an empty dataframe column 'token_length'\n",
    "    conversation.loc[:,'token_length'] = np.nan\n",
    "    \n",
    "    sum_token_length = 0\n",
    "    \n",
    "    for i in range(1, len(conversation)):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            # Tokenize speaker response and filter punctuations\n",
    "            tokens = nltk.word_tokenize(conversation[\"text\"][i])\n",
    "            tokens = list(filter(lambda tokens: tokens not in string.punctuation, tokens)) # TOIMPROVE: exclude quotation marks\n",
    "            \n",
    "            # Add token length to dataframe\n",
    "            conversation.at[i,'token_length'] = len(tokens)\n",
    "            \n",
    "            # Calculate sum of all tokens by speaker\n",
    "            sum_token_length += len(tokens)\n",
    "        \n",
    "    return conversation, sum_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_speaker_responses(conversation, speaker, listener):\n",
    "    i = 0\n",
    "    \n",
    "    while i < (len(conversation) - 1):\n",
    "        if conversation['author'].iloc[i] == speaker and conversation['author'].iloc[i+1] == speaker:\n",
    "            if (conversation['dialog_turn'].iloc[i] + 1) == conversation['dialog_turn'].iloc[i+1]:\n",
    "                dropped_turn = conversation['dialog_turn'].iloc[i+1]\n",
    "                former_text = conversation['text'][i]\n",
    "                latter_text = conversation['text'][i+1]\n",
    "\n",
    "                # Merge consecutive responses of the speaker\n",
    "                merged_text = former_text + \" \" + latter_text \n",
    "                conversation['text'].replace({former_text: merged_text}, inplace=True)\n",
    "\n",
    "                # Get names of indexes for which column dialog_turn has value of the dropped turn\n",
    "                conversation_index_names = conversation[conversation['dialog_turn'] == dropped_turn].index\n",
    "\n",
    "                # Delete these row indexes from dataframe\n",
    "                conversation.drop(conversation_index_names, inplace=True)\n",
    "\n",
    "                # Reset indexes\n",
    "                conversation.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "        i += 1\n",
    "        \n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    listener_responses = conversation[conversation[\"author\"] == listener]\n",
    "    num_speaker_responses = len(speaker_responses) \n",
    "    num_listener_responses = len(listener_responses)\n",
    "                 \n",
    "    return conversation, num_speaker_responses, num_listener_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if conversation is interleaved. \n",
    "# If all the even-numbered turns are by the speaker and if all the odd-numbered turns are by the listener, returns True. Otherwise, returns False.\n",
    "def is_interleaved_conversation(speaker, listener, conversation):\n",
    "    num_turns = len(conversation)\n",
    "    \n",
    "    for i in range(0, num_turns, 2):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    for i in range(1, num_turns, 2):\n",
    "        if conversation['author'].iloc[i] == listener:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engagement_preprocessing(speaker, listener, conversation):\n",
    "    conversation, num_speaker_responses, num_listener_responses = merge_consecutive_speaker_responses(conversation, speaker, listener)\n",
    "    conversation, sum_token_length = calculate_speaker_token_length(conversation, speaker)\n",
    "    num_turn_diff = abs(num_speaker_responses - num_listener_responses)\n",
    "    interleaved = is_interleaved_conversation(speaker, listener, conversation)\n",
    "    num_turns = len(conversation)\n",
    "\n",
    "    # Set an upper limit to token length's effect on engagement\n",
    "    if sum_token_length >= 30:\n",
    "        token_length_score = 30\n",
    "    else:\n",
    "        token_length_score = sum_token_length\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    if interleaved == True:\n",
    "        interleaved_int = 1\n",
    "    else:\n",
    "        interleaved_int = -1\n",
    "        \n",
    "    return num_turns, interleaved, token_length_score, num_turn_diff, conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_sentiment(conversation, speaker):   \n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    sns.set_theme(style=\"white\")\n",
    "    g = sns.relplot(x=\"dialog_turn\", y=\"strongest_compound\", hue=\"sentiment\", style=\"emotion_prediction\", palette=\"Set1\",data=speaker_responses, s=200)\n",
    "    g.fig.suptitle('Sentimental and Emotional Shift in Speaker Responses with Conversation Progression', fontsize=16)\n",
    "    g.fig.subplots_adjust(top=0.9);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all emotions and the final emotion of the given responses\n",
    "def get_emotion_prediction(speaker_responses):\n",
    "    emotions = speaker_responses[\"emotion_prediction\"]\n",
    "    final_emotion = emotions.iloc[-1]\n",
    "    \n",
    "    return emotions, final_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all sentiments and the final sentiment of the given responses\n",
    "def get_sentiment(speaker_responses):\n",
    "    sentiments = speaker_responses[\"sentiment\"]\n",
    "    final_sentiment = sentiments.iloc[-1]\n",
    "    \n",
    "    return sentiments, final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the last speaker emotion is grateful and its sentiment is positive. If yes, return true. Otherwise, return false.\n",
    "def is_tagged_grateful_positive(speaker_responses):\n",
    "    _, final_sentiment = get_sentiment(speaker_responses)\n",
    "    _, final_emotion = get_emotion_prediction(speaker_responses)\n",
    "    \n",
    "    if final_sentiment == \"positive\" and final_emotion == \"grateful\":\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_toward_listener(speaker_response):    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    phrases = ['you', 'your']\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('toward_listener', None, *patterns)\n",
    "    sentence = nlp (speaker_response)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    " \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain profanity towards the listener\n",
    "def contains_profanity(conversation, speaker):\n",
    "    for i in range(1,len(conversation)):\n",
    "        if conversation['author'].iloc[i] == speaker:\n",
    "            for j in range(0,len(conversation['sentences'].iloc[i])):\n",
    "                # i'th dialogue turn, j'th sentence\n",
    "                if predict([conversation['sentences'][i][j]]) == 1 and is_toward_listener(conversation['sentences'][i][j]) == True:\n",
    "                    # uncomment to print the sentence that contains profanity\n",
    "                    #print(conversation['sentences'][i][j])\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_gratitude(conversation, speaker):\n",
    " \n",
    "    # Take the speaker responses except first one\n",
    "    speaker_responses = conversation[conversation['author'] == speaker]\n",
    "    speaker_responses = speaker_responses[speaker_responses['dialog_turn'] != 1]\n",
    "    speaker_responses = speaker_responses['text']\n",
    "    speaker_responses = speaker_responses.to_string()[1:].lower()\n",
    "    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "   \n",
    "    phrases = ['thank', 'means a lot to me', 'thanks', 'appreciate', 'support', 'concern'\n",
    "               'your help', 'means so much to me', 'grateful', 'kind of you', 'repay you', \n",
    "               'taking the time']\n",
    "\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('gratitude', None, *patterns)\n",
    "    sentence = nlp (speaker_responses)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    "    \n",
    "    # uncomment this part if you want to print the matched phrases\n",
    "    #for match_id, start, end in matched_phrases:\n",
    "        #string_id = nlp.vocab.strings[match_id]  \n",
    "        #span = sentence[start:end]                   \n",
    "        #print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain sarcasm\n",
    "def contains_sarcasm(conversation, speaker, tokenizer, model):\n",
    "    \n",
    "    # Take the speaker responses except first one\\\n",
    "    speaker_sentences = conversation[conversation['author'] == speaker]\n",
    "    speaker_sentences = speaker_sentences[speaker_sentences['dialog_turn'] != 1]\n",
    "    speaker_sentences = speaker_sentences['sentences']\n",
    "    sarcastic_probas = sarcastic.proba(speaker_sentences, tokenizer, model)\n",
    "    #print(sarcastic_probas)\n",
    "    \n",
    "    # Can be optimized \n",
    "    if (sarcastic_probas > 0.6).any():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if any of the speaker responses, except the first, contain disagreement\n",
    "def contains_disagreement(conversation, speaker):\n",
    "        \n",
    "    # Take the speaker responses except first one\n",
    "    speaker_responses = conversation[conversation['author'] == speaker]\n",
    "    speaker_responses = speaker_responses[speaker_responses['dialog_turn'] != 1]\n",
    "    speaker_responses = speaker_responses['text']\n",
    "    speaker_responses = speaker_responses.to_string()[1:].lower()\n",
    "    \n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "   \n",
    "    phrases = [\"i don't think so\", \"no way\", \"disagree\", \"i beg to differ\", \"i'd say the exact opposite\", \n",
    "               \"not necessarily\", \"that's not always true\", \"that's not always the case\", \"i'm not so sure about that\", \n",
    "               \"that doesn’t make much sense to me\", \"i don’t share your view\", \"i don’t agree with you\"]\n",
    "\n",
    "    patterns = [nlp(text) for text in phrases]\n",
    "    phrase_matcher.add('disagreement', None, *patterns)\n",
    "    sentence = nlp (speaker_responses)\n",
    "    matched_phrases = phrase_matcher(sentence)\n",
    "    \n",
    "    # uncomment this part if you want to print the matched phrases\n",
    "    #for match_id, start, end in matched_phrases:\n",
    "        #string_id = nlp.vocab.strings[match_id]  \n",
    "        #span = sentence[start:end]                   \n",
    "        #print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "    if len(matched_phrases) > 0:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column with sentence-level sentiment compounds\n",
    "def sentence_level_sentiment(conversation):\n",
    "    conversation['sentences'] = conversation['text'].apply(lambda x: sent_tokenize(x))\n",
    "    conversation['sentences'] = conversation['sentences'].map(lambda x: list(map(str.lower, x)))\n",
    "    conversation['sentence_compounds'] = conversation['sentences']\n",
    "    \n",
    "    for i in range(0,len(conversation)):\n",
    "        num_sentences = len(sent_tokenize(conversation['text'].iloc[i]))\n",
    "        # sentiment compound for each sentence\n",
    "        scores = np.zeros(num_sentences) \n",
    "        for j in range(0,num_sentences):\n",
    "            # i'th dialogue turn, j'th sentence\n",
    "            scores[j] = sid.polarity_scores(sent_tokenize(conversation['text'][i])[j])['compound']\n",
    "\n",
    "            conversation['sentence_compounds'][i] = scores\n",
    "            \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a column with the sentence compound with strongest magnitude within a dialogue turn\n",
    "def strongest_sentiment(conversation):\n",
    "    conversation['strongest_compound'] = conversation['sentence_compounds']\n",
    "    conversation['strongest_compound'] = conversation['strongest_compound'].apply(lambda x: np.min(x) if np.max(abs(x)) == abs(np.min(x)) else np.max(x))\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IN LATER WEEKS?\n",
    "def emobert_predict_sentence_emotion(conversation):\n",
    "    conversation['sentence_level_emotion_prediction'] = conversation['sentences']\n",
    "    \n",
    "    for i in range(0,len(conversation)):\n",
    "        num_sentences = len(sent_tokenize(conversation['text'].iloc[i]))\n",
    "        # emotion for each sentence\n",
    "        emotions = np.zeros(num_sentences) \n",
    "        for j in range(0,num_sentences):\n",
    "            # i'th dialogue turn, j'th sentence\n",
    "            emotions[j] = sent_tokenize(conversation['text'][i])[j].swifter.apply(emobert_predict_emotion)\n",
    "            conversation['sentence_level_emotion_prediction'][i] = emotions\n",
    "            \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satisfaction_preprocessing(conversation, speaker, tokenizer, model):\n",
    "    conversation = sentence_level_sentiment(conversation)\n",
    "    conversation = strongest_sentiment(conversation)\n",
    "    speaker_responses = conversation[conversation[\"author\"] == speaker]\n",
    "    grateful_bonus = 0\n",
    "    profanity_penalty = 0\n",
    "    sarcasm_penalty = 0\n",
    "    disagreement_penalty = 0\n",
    "    \n",
    "    # Change in sentiment from the first to the last turn\n",
    "    sentiment_change = speaker_responses['strongest_compound'].iloc[-1] - speaker_responses['strongest_compound'].iloc[0]\n",
    "    \n",
    "    # Take the slope of the compounds of speaker responses\n",
    "    f = np.polyfit(speaker_responses['dialog_turn'], speaker_responses['compound'], deg=1)\n",
    "    slope = f[0]\n",
    "\n",
    "    if is_tagged_grateful_positive(speaker_responses)==True or contains_gratitude(speaker_responses, speaker)==True:\n",
    "        grateful_bonus = 1\n",
    "        \n",
    "    if contains_profanity(conversation, speaker) == True:\n",
    "        profanity_penalty = -1\n",
    "    \n",
    "    if contains_sarcasm(conversation, speaker, tokenizer, model) == True:\n",
    "        sarcasm_penalty = -1    \n",
    "    \n",
    "    if contains_disagreement(conversation, speaker) == True:\n",
    "        disagreement_penalty = -1\n",
    "        \n",
    "    return slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on the randomly selected 100 dialogues from 8 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sarcastic model \n",
    "tokenizer, model = sarcastic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_satisfaction = {'slope_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sentiment_change_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'grateful_bonus_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'], \n",
    "                          'profanity_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sarcasm_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'disagreement_penalty_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                          'sat_threshold': ['1.25', '1.5', '1.75', '2.0', '2.25'],\n",
    "                          'sarcastic_probas': ['0.3', '0.4', '0.5', '0.6', '0.7']}\n",
    "\n",
    "param_grid_engagement = {'num_turns_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                        'interleaved_weight': ['0.2', '0.4', '0.6', '0.8', '1.0'],\n",
    "                        'token_length_weight': ['0.02', '0.04', '0.06', '0.08', '0.10'],\n",
    "                        'diff_weight': ['-0.2', '-0.4', '-0.6', '-0.8', '-1.0'],\n",
    "                        'eng_threshold': ['3.25', '3.5', '3.75', '4.0', '4.25']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement hyperparameters\n",
    "eng_threshold = 4\n",
    "num_turns_weight = 1\n",
    "interleaved_weight = 1\n",
    "token_length_weight = 0.05\n",
    "diff_weight = -1\n",
    "\n",
    "# Satisfaction hyperparameters\n",
    "sat_threshold = 0.7\n",
    "slope_weight = 1\n",
    "sentiment_change_weight = 1\n",
    "grateful_bonus_weight = 2\n",
    "profanity_penalty_weight = 1\n",
    "sarcasm_penalty_weight = 1\n",
    "disagreement_penalty_weight = 1\n",
    "\n",
    "# Put these in the parameters of the functions below\n",
    "# Check out how to pass parameters with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions dataframe \n",
    "grouped = df.groupby(['conversation_id','subreddit']).groups\n",
    "cols = df.columns.tolist()\n",
    "df_train_predictions = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Train-test split\n",
    "s = pd.Series(grouped)\n",
    "train, test  = [i.to_dict() for i in train_test_split(s, train_size=0.8)]\n",
    "\n",
    "for conv_id, subreddit in train:\n",
    "    conversation, speaker, listener = extract_responses(conv_id, subreddit)\n",
    "    \n",
    "    ###--- Predict engagement ---###\n",
    "    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "    engagement_score = num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*num_turn_diff\n",
    "    engagement = 1 if engagement_score >= eng_threshold else 0\n",
    "    conversation['predicted_engagement'] = engagement\n",
    "    ###------------------------------###\n",
    "    \n",
    "    ###--- Predict satisfaction ---###\n",
    "    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "    satisfaction_score = slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty\n",
    "    satisfaction = 1 if satisfaction_score >= sat_threshold else 0\n",
    "    conversation['predicted_satisfaction'] = satisfaction\n",
    "    ###------------------------------###\n",
    "    \n",
    "    df_train_predictions = df_train_predictions.append(conversation)\n",
    "\n",
    "df_train_predictions = df_train_predictions[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', \n",
    "                'predicted_satisfaction', 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "    \n",
    "df_train_predictions.to_csv(\"data/RED/annotated/train_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(TP,FP,TN,FN):\n",
    "    P = TP/(TP+FP)\n",
    "    R = TP/(TP+FN)\n",
    "    f1 = 2*P*R/(P+R)\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "        \n",
    "    return P, R, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first utterance of each conversation\n",
    "first_utterances = df_train_predictions.groupby(['conversation_id', 'subreddit']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SATISFACTION -- precision:  0.82  recall:  0.78  f1-score:  0.8  accuracy:  0.74\n"
     ]
    }
   ],
   "source": [
    "TP_s = 0\n",
    "TN_s = 0\n",
    "FP_s = 0\n",
    "FN_s = 0\n",
    "\n",
    "# Compare satisfaction predictions with ground truths \n",
    "for i in range(0,len(first_utterances)):\n",
    "    if ((first_utterances.iloc[i]['ground_truth_satisfaction'] == 1) & (first_utterances.iloc[i]['predicted_satisfaction'] == 1)).all():\n",
    "        TP_s += 1\n",
    "    elif ((first_utterances.iloc[i]['ground_truth_satisfaction'] == 0) & (first_utterances.iloc[i]['predicted_satisfaction'] == 1)).all():\n",
    "        FP_s += 1\n",
    "    elif ((first_utterances.iloc[i]['ground_truth_satisfaction'] == 1) & (first_utterances.iloc[i]['predicted_satisfaction'] == 0)).all():\n",
    "        FN_s += 1\n",
    "    else:\n",
    "        TN_s += 1\n",
    "    \n",
    "P_s, R_s, f1_s, acc_s = performance(TP_s, FP_s, TN_s, FN_s)\n",
    "\n",
    "print(\"SATISFACTION -- precision: \", round_sig(P_s), \" recall: \", round_sig(R_s), \" f1-score: \", round_sig(f1_s), \" accuracy: \", round_sig(acc_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGAGEMENT -- precision:  0.88  recall:  0.85  f1-score:  0.87  accuracy:  0.76\n"
     ]
    }
   ],
   "source": [
    "TP_e = 0\n",
    "TN_e = 0\n",
    "FP_e = 0\n",
    "FN_e = 0\n",
    "\n",
    "# Compare engagement predictions with ground truths\n",
    "for i in range(0,len(first_utterances)):\n",
    "    if ((first_utterances.iloc[i]['ground_truth_engagement'] == 1) & (first_utterances.iloc[i]['predicted_engagement'] == 1)).all():\n",
    "        TP_e += 1\n",
    "    elif ((first_utterances.iloc[i]['ground_truth_engagement'] == 0) & (first_utterances.iloc[i]['predicted_engagement'] == 1)).all():\n",
    "        FP_e += 1\n",
    "    elif ((first_utterances.iloc[i]['ground_truth_engagement'] == 1) & (first_utterances.iloc[i]['predicted_engagement'] == 0)).all():\n",
    "        FN_e += 1\n",
    "    else:\n",
    "        TN_e += 1\n",
    "        \n",
    "P_e, R_e, f1_e, acc_e = performance(TP_e, FP_e, TN_e, FN_e)\n",
    "\n",
    "print(\"ENGAGEMENT -- precision: \", round_sig(P_e), \" recall: \", round_sig(R_e), \" f1-score: \", round_sig(f1_e), \" accuracy: \", round_sig(acc_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
