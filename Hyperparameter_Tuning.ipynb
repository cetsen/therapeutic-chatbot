{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import helpers\n",
    "import sarcastic\n",
    "from engagement import engagement_preprocessing\n",
    "from satisfaction import satisfaction_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Display long column text\n",
    "pd.options.display.max_colwidth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sarcasm classification model \n",
    "tokenizer, model = sarcastic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RED/annotated/100_annotated_dialogues_2.csv\")\n",
    "df = df.rename(columns={'conversation id': 'conversation_id', 'post title': 'post_title', 'dialog turn': 'dialog_turn', 'emotion prediction': 'emotion_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement hyperparameters\n",
    "eng_threshold = [3]\n",
    "num_turns_weight = [1]\n",
    "interleaved_weight = [1]\n",
    "token_length_weight = [0.05]\n",
    "diff_weight = [-0.5]\n",
    "\n",
    "# Satisfaction hyperparameters\n",
    "sat_threshold = [0.5] \n",
    "slope_weight = [0.5]\n",
    "sentiment_change_weight = [0.5]\n",
    "grateful_bonus_weight = [3] \n",
    "profanity_penalty_weight = [0.5]\n",
    "sarcasm_penalty_weight = [0.5]\n",
    "disagreement_penalty_weight = [0.5]\n",
    "\n",
    "hp = {\n",
    "    \"eng_threshold\": eng_threshold,\n",
    "    \"num_turns_weight\": num_turns_weight,\n",
    "    \"interleaved_weight\": interleaved_weight,\n",
    "    \"token_length_weight\": token_length_weight,\n",
    "    \"diff_weight\": diff_weight,\n",
    "    \"sat_threshold\": sat_threshold,\n",
    "    \"slope_weight\": slope_weight,\n",
    "    \"sentiment_change_weight\": sentiment_change_weight,\n",
    "    \"grateful_bonus_weight\": grateful_bonus_weight,\n",
    "    \"profanity_penalty_weight\": profanity_penalty_weight,\n",
    "    \"sarcasm_penalty_weight\": sarcasm_penalty_weight,\n",
    "    \"disagreement_penalty_weight\": disagreement_penalty_weight   \n",
    "}\n",
    "\n",
    "# Names (keys) of hyperparameters\n",
    "n_hp = list(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group conversations by conversation_id and subreddit\n",
    "grouped = df.groupby(['conversation_id', 'subreddit']).groups\n",
    "\n",
    "# Train-test split conversations\n",
    "s = pd.Series(grouped)\n",
    "val, test = [i.to_dict() for i in train_test_split(s, train_size=0.5, random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with hyperparameters:  3 1 1 0.05 -0.5 0.5 0.5 0.5 3 0.5 0.5 0.5\n",
      "False negative satisfaction 398\n",
      "False negative satisfaction 3183\n",
      "False positive satisfaction 5024\n",
      "False negative satisfaction 6836\n",
      "False negative satisfaction 8314\n",
      "False negative satisfaction 9854\n",
      "False negative satisfaction 10633\n",
      "False negative satisfaction 50030\n",
      "False negative satisfaction 69061\n",
      "False positive satisfaction 75539\n",
      "False positive satisfaction 114807\n",
      "False negative satisfaction 239904\n",
      "False negative satisfaction 278531\n",
      "Current best engagement hyperparameters are : {'eng_threshold': 3, 'num_turns_weight': 1, 'interleaved_weight': 1, 'token_length_weight': 0.05, 'diff_weight': -0.5, 'P_e': 0.92, 'R_e': 1.0, 'f1_e': 0.9583333333333334, 'acc_e': 0.92}\n",
      "Current best satisfaction hyperparameters are : {'sat_threshold': 0.5, 'slope_weight': 0.5, 'sentiment_change_weight': 0.5, 'grateful_bonus_weight': 3, 'profanity_penalty_weight': 0.5, 'sarcasm_penalty_weight': 0.5, 'disagreement_penalty_weight': 0.5, 'P_s': 0.896551724137931, 'R_s': 0.7222222222222222, 'f1_s': 0.7999999999999999, 'acc_s': 0.74}\n",
      "Time it takes for grid search (in seconds):  63.31794285774231\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "start = time.time()\n",
    "cols = df.columns.tolist()\n",
    "df_val_preds = pd.DataFrame(columns=cols)\n",
    "\n",
    "all_params_e, all_params_s = [], []\n",
    "best_params_e, best_params_s, curr_params_e, curr_params_s = {}, {}, {}, {}\n",
    "best_e, best_s = 0, 0\n",
    "\n",
    "for p1 in hp[n_hp[0]]:\n",
    "    for p2 in hp[n_hp[1]]:\n",
    "        for p3 in hp[n_hp[2]]:\n",
    "            for p4 in hp[n_hp[3]]:\n",
    "                for p5 in hp[n_hp[4]]:\n",
    "                    for p6 in hp[n_hp[5]]:\n",
    "                        for p7 in hp[n_hp[6]]:\n",
    "                            for p8 in hp[n_hp[7]]:\n",
    "                                for p9 in hp[n_hp[8]]:\n",
    "                                    for p10 in hp[n_hp[9]]:\n",
    "                                        for p11 in hp[n_hp[10]]:\n",
    "                                            for p12 in hp[n_hp[11]]:\n",
    "                                                print('Testing with hyperparameters: ', p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12)\n",
    "                                                for conv_id, subreddit in val:\n",
    "                                                    conversation, speaker, listener = helpers.extract_responses(conv_id, subreddit, df)\n",
    "\n",
    "                                                    # Predict engagement\n",
    "                                                    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "                                                    engagement_score = p2*num_turns + p3*interleaved + p4*token_length_score + p5*num_turn_diff\n",
    "                                                    engagement = 1 if engagement_score >= p1 else 0\n",
    "                                                    conversation['predicted_engagement'] = engagement\n",
    "\n",
    "                                                    # Predict satisfaction\n",
    "                                                    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "                                                    satisfaction_score = p7*slope + p8*sentiment_change + p9*grateful_bonus + p10*profanity_penalty + p11*sarcasm_penalty + p12*disagreement_penalty\n",
    "                                                    satisfaction = 1 if satisfaction_score >= p6 else 0\n",
    "                                                    conversation['predicted_satisfaction'] = satisfaction\n",
    "\n",
    "                                                    df_val_preds = df_val_preds.append(conversation)\n",
    "\n",
    "                                                # Reorder columns\n",
    "                                                df_val_preds = df_val_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', 'predicted_satisfaction',\\\n",
    "                                                                             'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "\n",
    "                                                # Take first utterance of each conversation to get predictions and labels\n",
    "                                                first_utters_val = df_val_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "\n",
    "                                                # Compare predictions to labels and return scores\n",
    "                                                P_s, R_s, f1_s, acc_s, P_e, R_e, f1_e, acc_e = helpers.test(first_utters_val)\n",
    "\n",
    "                                                # Current parameters\n",
    "                                                curr_params_e[n_hp[0]] = p1\n",
    "                                                curr_params_e[n_hp[1]] = p2\n",
    "                                                curr_params_e[n_hp[2]] = p3\n",
    "                                                curr_params_e[n_hp[3]] = p4\n",
    "                                                curr_params_e[n_hp[4]] = p5\n",
    "                                                curr_params_e['P_e'] = P_e\n",
    "                                                curr_params_e['R_e'] = R_e\n",
    "                                                curr_params_e['f1_e'] = f1_e\n",
    "                                                curr_params_e['acc_e'] = acc_e\n",
    "                                                curr_params_s[n_hp[5]] = p6\n",
    "                                                curr_params_s[n_hp[6]] = p7\n",
    "                                                curr_params_s[n_hp[7]] = p8\n",
    "                                                curr_params_s[n_hp[8]] = p9\n",
    "                                                curr_params_s[n_hp[9]] = p10\n",
    "                                                curr_params_s[n_hp[10]] = p11\n",
    "                                                curr_params_s[n_hp[11]] = p12\n",
    "                                                curr_params_s['P_s'] = P_s\n",
    "                                                curr_params_s['R_s'] = R_s\n",
    "                                                curr_params_s['f1_s'] = f1_s\n",
    "                                                curr_params_s['acc_s'] = acc_s\n",
    "\n",
    "                                                # Append current parameters to list of all parameters\n",
    "                                                all_params_e.append(curr_params_e.copy())\n",
    "                                                all_params_s.append(curr_params_s.copy())\n",
    "\n",
    "                                                # Update best parameters\n",
    "                                                if f1_e > best_e:\n",
    "                                                    best_e = f1_e\n",
    "                                                    best_params_e = curr_params_e\n",
    "                                                    print('Current best engagement hyperparameters are :', best_params_e)\n",
    "\n",
    "                                                if f1_s > best_s:\n",
    "                                                    best_s = f1_s\n",
    "                                                    best_params_s = curr_params_s\n",
    "                                                    print('Current best satisfaction hyperparameters are :', best_params_s)\n",
    "                                                    \n",
    "end = time.time()\n",
    "print('Time it takes for grid search (in seconds): ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameters/best_params_engagement.txt','w') as data:\n",
    "    data.write(str(best_params_e))\n",
    "    \n",
    "with open('hyperparameters/best_params_satisfaction.txt','w') as data:\n",
    "    data.write(str(best_params_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameters/all_params_engagement.txt','w') as data:\n",
    "    data.write(str(all_params_e))\n",
    "    \n",
    "with open('hyperparameters/all_params_satisfaction.txt','w') as data:\n",
    "    data.write(str(all_params_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### TEST ###\n",
    "# TODO: Give best hyperparameters found in validation\n",
    "# TODO: Predict for the test set:\n",
    "for conv_id, subreddit in test:\n",
    "    conversation, speaker, listener = extract_responses(conv_id, subreddit)\n",
    "    \n",
    "    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "    engagement_score = num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*num_turn_diff\n",
    "    engagement = 1 if engagement_score >= eng_threshold else 0\n",
    "    conversation['predicted_engagement'] = engagement\n",
    "    \n",
    "    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "    satisfaction_score = slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty\n",
    "    satisfaction = 1 if satisfaction_score >= sat_threshold else 0\n",
    "    conversation['predicted_satisfaction'] = satisfaction\n",
    "    \n",
    "    df_test_preds = df_test_preds.append(conversation)\n",
    "\n",
    "df_test_preds = df_test_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', \n",
    "                'predicted_satisfaction', 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "df_test_preds.to_csv(\"data/RED/annotated/test_predictions.csv\", index=False)\n",
    "\n",
    "df_test_preds = df_test_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "# TODO: Test performance of test set:\n",
    "test_P_s, test_R_s, test_f1_s, test_acc_s, test_P_e, test_R_e, test_f1_e, test_acc_e = helpers.test(first_utters_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEPS: \n",
    "* Try grid search with a subset of the hyperparameters\n",
    "* Apply algorithm with best values to entire dyadic dataset\n",
    "* Retrain emobert on whole dataset on sentence level\n",
    "* Write guidelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
