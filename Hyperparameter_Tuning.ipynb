{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LinearSVC from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.24.0 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import helpers\n",
    "import sarcastic\n",
    "from engagement import engagement_preprocessing\n",
    "from satisfaction import satisfaction_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Display long column text\n",
    "pd.options.display.max_colwidth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sarcasm classification model \n",
    "tokenizer, model = sarcastic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RED/annotated/100_annotated_dialogues.csv\")\n",
    "df = df.rename(columns={'conversation id': 'conversation_id', 'post title': 'post_title', 'dialog turn': 'dialog_turn', 'emotion prediction': 'emotion_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement hyperparameters\n",
    "eng_threshold = [2, 3, 4]\n",
    "num_turns_weight = [0.5, 1, 1.5]\n",
    "interleaved_weight = [0.5, 1, 1.5]\n",
    "token_length_weight = [0.01, .05, 0.25]\n",
    "diff_weight = [-0.1, -0.5, -1]\n",
    "\n",
    "# Satisfaction hyperparameters\n",
    "sat_threshold = [0.5, 0.75, 1]\n",
    "slope_weight = [0.5, 0.75, 1]\n",
    "sentiment_change_weight = [0.5, 0.75, 1]\n",
    "grateful_bonus_weight = [1.5, 2, 2.5]\n",
    "profanity_penalty_weight = [0.5, 0.75, 1]\n",
    "sarcasm_penalty_weight = [0.5, 0.75, 1]\n",
    "disagreement_penalty_weight = [0.5, 0.75, 1]\n",
    "\n",
    "hp = {\n",
    "    \"eng_threshold\": eng_threshold,\n",
    "    \"num_turns_weight\": num_turns_weight,\n",
    "    \"interleaved_weight\": interleaved_weight,\n",
    "    \"token_length_weight\": token_length_weight,\n",
    "    \"diff_weight\": diff_weight,\n",
    "    \"sat_threshold\": sat_threshold,\n",
    "    \"slope_weight\": slope_weight,\n",
    "    \"sentiment_change_weight\": sentiment_change_weight,\n",
    "    \"grateful_bonus_weight\": grateful_bonus_weight,\n",
    "    \"profanity_penalty_weight\": profanity_penalty_weight,\n",
    "    \"sarcasm_penalty_weight\": sarcasm_penalty_weight,\n",
    "    \"disagreement_penalty_weight\": disagreement_penalty_weight   \n",
    "}\n",
    "\n",
    "# Names (keys) of hyperparameters\n",
    "n_hp = list(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group conversations by conversation_id and subreddit\n",
    "grouped = df.groupby(['conversation_id', 'subreddit']).groups\n",
    "\n",
    "# Train-test split conversations\n",
    "s = pd.Series(grouped)\n",
    "val, test = [i.to_dict() for i in train_test_split(s, train_size=0.5, random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with hyperparameters:  2 0.5 0.5 0.01 -0.1 0.5 0.5 0.5 1.5 0.5 0.5 0.5\n",
      "Current best hyperparameters are : {'eng_threshold': 2, 'num_turns_weight': 0.5, 'interleaved_weight': 0.5, 'token_length_weight': 0.01, 'diff_weight': -0.1, 'P_e': 0.9111111111111111, 'R_e': 0.8913043478260869, 'f1_e': 0.9010989010989011, 'acc_e': 0.82}\n",
      "Current best hyperparameters are : {'sat_threshold': 0.5, 'slope_weight': 0.5, 'sentiment_change_weight': 0.5, 'grateful_bonus_weight': 1.5, 'profanity_penalty_weight': 0.5, 'sarcasm_penalty_weight': 0.5, 'disagreement_penalty_weight': 0.5, 'P_s': 0.8235294117647058, 'R_s': 0.7777777777777778, 'f1_s': 0.7999999999999999, 'acc_s': 0.72}\n",
      "Testing with hyperparameters:  3 1 1 0.05 -0.5 0.75 0.75 0.75 2 0.75 0.75 0.75\n",
      "Testing with hyperparameters:  4 1.5 1.5 0.25 -1 1 1 1 2.5 1 1 1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "cols = df.columns.tolist()\n",
    "df_val_preds = pd.DataFrame(columns=cols)\n",
    "\n",
    "all_params_e, all_params_s = [], []\n",
    "best_params_e, best_params_s, curr_params_e, curr_params_s = {}, {}, {}, {}\n",
    "best_e, best_s = 0, 0\n",
    "\n",
    "for p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12 in zip(hp[n_hp[0]], hp[n_hp[1]], hp[n_hp[2]], hp[n_hp[3]], hp[n_hp[4]], hp[n_hp[5]], hp[n_hp[6]], hp[n_hp[7]], hp[n_hp[8]], hp[n_hp[9]], hp[n_hp[10]], hp[n_hp[11]]): \n",
    "    print('Testing with hyperparameters: ', p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12)\n",
    "    for conv_id, subreddit in val:\n",
    "        conversation, speaker, listener = helpers.extract_responses(conv_id, subreddit, df)\n",
    "\n",
    "        # Predict engagement\n",
    "        num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "        engagement_score = p2*num_turns + p3*interleaved + p4*token_length_score + p5*num_turn_diff\n",
    "        engagement = 1 if engagement_score >= p1 else 0\n",
    "        conversation['predicted_engagement'] = engagement\n",
    "\n",
    "        # Predict satisfaction\n",
    "        slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "        satisfaction_score = p7*slope + p8*sentiment_change + p9*grateful_bonus + p10*profanity_penalty + p11*sarcasm_penalty + p12*disagreement_penalty\n",
    "        satisfaction = 1 if satisfaction_score >= p6 else 0\n",
    "        conversation['predicted_satisfaction'] = satisfaction\n",
    "\n",
    "        df_val_preds = df_val_preds.append(conversation)\n",
    "    \n",
    "    # Reorder columns\n",
    "    df_val_preds = df_val_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', 'predicted_satisfaction',\\\n",
    "                                 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "    \n",
    "    # Take first utterance of each conversation to get predictions and labels\n",
    "    first_utters_val = df_val_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "    \n",
    "    # Compare predictions to labels and return scores\n",
    "    P_s, R_s, f1_s, acc_s, P_e, R_e, f1_e, acc_e = helpers.test(first_utters_val)\n",
    "    \n",
    "    # Current parameters\n",
    "    curr_params_e[n_hp[0]] = p1\n",
    "    curr_params_e[n_hp[1]] = p2\n",
    "    curr_params_e[n_hp[2]] = p3\n",
    "    curr_params_e[n_hp[3]] = p4\n",
    "    curr_params_e[n_hp[4]] = p5\n",
    "    curr_params_e['P_e'] = P_e\n",
    "    curr_params_e['R_e'] = R_e\n",
    "    curr_params_e['f1_e'] = f1_e\n",
    "    curr_params_e['acc_e'] = acc_e\n",
    "    curr_params_s[n_hp[5]] = p6\n",
    "    curr_params_s[n_hp[6]] = p7\n",
    "    curr_params_s[n_hp[7]] = p8\n",
    "    curr_params_s[n_hp[8]] = p9\n",
    "    curr_params_s[n_hp[9]] = p10\n",
    "    curr_params_s[n_hp[10]] = p11\n",
    "    curr_params_s[n_hp[11]] = p12\n",
    "    curr_params_s['P_s'] = P_s\n",
    "    curr_params_s['R_s'] = R_s\n",
    "    curr_params_s['f1_s'] = f1_s\n",
    "    curr_params_s['acc_s'] = acc_s\n",
    "    \n",
    "    # Append current parameters to list of all parameters\n",
    "    all_params_e.append(curr_params_e.copy())\n",
    "    all_params_s.append(curr_params_s.copy())\n",
    "    \n",
    "    # Update best parameters\n",
    "    if f1_e > best_e:\n",
    "        best_e = f1_e\n",
    "        best_params_e = curr_params_e\n",
    "        print('Current best hyperparameters are :', best_params_e)\n",
    "        \n",
    "    if f1_s > best_s:\n",
    "        best_s = f1_s\n",
    "        best_params_s = curr_params_s\n",
    "        print('Current best hyperparameters are :', best_params_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameters/best_params_engagement.txt','w') as data:\n",
    "    data.write(str(best_params_e))\n",
    "    \n",
    "with open('hyperparameters/best_params_satisfaction.txt','w') as data:\n",
    "    data.write(str(best_params_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameters/all_params_engagement.txt','w') as data:\n",
    "    data.write(str(all_params_e))\n",
    "    \n",
    "with open('hyperparameters/all_params_satisfaction.txt','w') as data:\n",
    "    data.write(str(all_params_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### TEST ###\\n# TODO: Give best hyperparameters found in validation\\n# TODO: Predict for the test set:\\nfor conv_id, subreddit in test:\\n    conversation, speaker, listener = extract_responses(conv_id, subreddit)\\n    \\n    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\\n    engagement_score = num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*num_turn_diff\\n    engagement = 1 if engagement_score >= eng_threshold else 0\\n    conversation[\\'predicted_engagement\\'] = engagement\\n    \\n    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\\n    satisfaction_score = slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty\\n    satisfaction = 1 if satisfaction_score >= sat_threshold else 0\\n    conversation[\\'predicted_satisfaction\\'] = satisfaction\\n    \\n    df_test_preds = df_test_preds.append(conversation)\\n\\ndf_test_preds = df_test_preds[[\\'conversation_id\\', \\'subreddit\\', \\'post_title\\', \\'author\\', \\'dialog_turn\\', \\'text\\', \\'ground_truth_satisfaction\\', \\'ground_truth_engagement\\', \\n                \\'predicted_satisfaction\\', \\'predicted_engagement\\', \\'compound\\', \\'sentiment\\', \\'emotion_prediction\\', \\'token_length\\', \\'sentences\\', \\'sentence_compounds\\', \\'strongest_compound\\']]\\ndf_test_preds.to_csv(\"data/RED/annotated/test_predictions.csv\", index=False)\\n\\ndf_test_preds = df_test_preds.groupby([\\'conversation_id\\', \\'subreddit\\']).first().reset_index()\\n# TODO: Test performance of test set:\\ntest_P_s, test_R_s, test_f1_s, test_acc_s, test_P_e, test_R_e, test_f1_e, test_acc_e = helpers.test(first_utters_test)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "### TEST ###\n",
    "# TODO: Give best hyperparameters found in validation\n",
    "# TODO: Predict for the test set:\n",
    "for conv_id, subreddit in test:\n",
    "    conversation, speaker, listener = extract_responses(conv_id, subreddit)\n",
    "    \n",
    "    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "    engagement_score = num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*num_turn_diff\n",
    "    engagement = 1 if engagement_score >= eng_threshold else 0\n",
    "    conversation['predicted_engagement'] = engagement\n",
    "    \n",
    "    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "    satisfaction_score = slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty\n",
    "    satisfaction = 1 if satisfaction_score >= sat_threshold else 0\n",
    "    conversation['predicted_satisfaction'] = satisfaction\n",
    "    \n",
    "    df_test_preds = df_test_preds.append(conversation)\n",
    "\n",
    "df_test_preds = df_test_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', \n",
    "                'predicted_satisfaction', 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "df_test_preds.to_csv(\"data/RED/annotated/test_predictions.csv\", index=False)\n",
    "\n",
    "df_test_preds = df_test_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "# TODO: Test performance of test set:\n",
    "test_P_s, test_R_s, test_f1_s, test_acc_s, test_P_e, test_R_e, test_f1_e, test_acc_e = helpers.test(first_utters_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEPS: \n",
    "1. Apply algorithm with best values to entire dyadic dataset\n",
    "2. Retrain emobert on whole dataset on sentence level\n",
    "3. Write guidelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
