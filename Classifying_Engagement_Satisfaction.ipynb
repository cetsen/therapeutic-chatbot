{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import helpers\n",
    "import sarcastic\n",
    "from engagement import engagement_preprocessing\n",
    "from satisfaction import satisfaction_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Display long column text\n",
    "pd.options.display.max_colwidth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sarcasm classification model \n",
    "tokenizer, model = sarcastic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RED/annotated/100_annotated_dialogues.csv\")\n",
    "df = df.rename(columns={'conversation id': 'conversation_id', 'post title': 'post_title', 'dialog turn': 'dialog_turn', 'emotion prediction': 'emotion_prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement hyperparameters\n",
    "eng_threshold = [2, 3, 4]\n",
    "num_turns_weight = [0.5, 1, 1.5]\n",
    "interleaved_weight = [0.5, 1, 1.5]\n",
    "token_length_weight = [0.01, .05, 0.25]\n",
    "diff_weight = [-0.1, -0.5, -1]\n",
    "\n",
    "# Satisfaction hyperparameters\n",
    "sat_threshold = [0.5, 0.75, 1]\n",
    "slope_weight = [0.5, 0.75, 1]\n",
    "sentiment_change_weight = [0.5, 0.75, 1]\n",
    "grateful_bonus_weight = [1.5, 2, 2.5]\n",
    "profanity_penalty_weight = [0.5, 0.75, 1]\n",
    "sarcasm_penalty_weight = [0.5, 0.75, 1]\n",
    "disagreement_penalty_weight = [0.5, 0.75, 1]\n",
    "\n",
    "hp = {\n",
    "    \"eng_threshold\": eng_threshold,\n",
    "    \"num_turns_weight\": num_turns_weight,\n",
    "    \"interleaved_weight\": interleaved_weight,\n",
    "    \"token_length_weight\": token_length_weight,\n",
    "    \"diff_weight\": diff_weight,\n",
    "    \"sat_threshold\": sat_threshold,\n",
    "    \"slope_weight\": slope_weight,\n",
    "    \"sentiment_change_weight\": sentiment_change_weight,\n",
    "    \"grateful_bonus_weight\": grateful_bonus_weight,\n",
    "    \"profanity_penalty_weight\": profanity_penalty_weight,\n",
    "    \"sarcasm_penalty_weight\": sarcasm_penalty_weight,\n",
    "    \"disagreement_penalty_weight\": disagreement_penalty_weight   \n",
    "}\n",
    "\n",
    "# Names of hyperparameters\n",
    "n_hp = list(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group conversations by conversation_id and subreddit\n",
    "grouped = df.groupby(['conversation_id', 'subreddit']).groups\n",
    "\n",
    "# Train-test split conversations\n",
    "s = pd.Series(grouped)\n",
    "val, test = [i.to_dict() for i in train_test_split(s, train_size=0.5, random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with hyperparameters:  3 1 1 0.05 -0.5 0.5 1 1 2 1 1 1\n",
      "Current best hyperparameters are : {'eng_threshold': 3, 'num_turns_weight': 1, 'interleaved_weight': 1, 'token_length_weight': 0.05, 'diff_weight': -0.5, 'P_e': 0.92, 'R_e': 1.0, 'f1_e': 0.9583333333333334, 'acc_e': 0.92}\n",
      "Current best hyperparameters are : {'eng_threshold': 3, 'num_turns_weight': 1, 'interleaved_weight': 1, 'token_length_weight': 0.05, 'diff_weight': -0.5, 'P_e': 0.92, 'R_e': 1.0, 'f1_e': 0.9583333333333334, 'acc_e': 0.92, 'sat_threshold': 0.5, 'slope_weight': 1, 'sentiment_change_weight': 1, 'grateful_bonus_weight': 2, 'profanity_penalty_weight': 1, 'sarcasm_penalty_weight': 1, 'disagreement_penalty_weight': 1, 'P_s': 0.7567567567567568, 'R_s': 0.7777777777777778, 'f1_s': 0.7671232876712328, 'acc_s': 0.66}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eng_threshold': 3,\n",
       " 'num_turns_weight': 1,\n",
       " 'interleaved_weight': 1,\n",
       " 'token_length_weight': 0.05,\n",
       " 'diff_weight': -0.5,\n",
       " 'P_e': 0.92,\n",
       " 'R_e': 1.0,\n",
       " 'f1_e': 0.9583333333333334,\n",
       " 'acc_e': 0.92,\n",
       " 'sat_threshold': 0.5,\n",
       " 'slope_weight': 1,\n",
       " 'sentiment_change_weight': 1,\n",
       " 'grateful_bonus_weight': 2,\n",
       " 'profanity_penalty_weight': 1,\n",
       " 'sarcasm_penalty_weight': 1,\n",
       " 'disagreement_penalty_weight': 1,\n",
       " 'P_s': 0.7567567567567568,\n",
       " 'R_s': 0.7777777777777778,\n",
       " 'f1_s': 0.7671232876712328,\n",
       " 'acc_s': 0.66}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "cols = df.columns.tolist()\n",
    "df_val_preds = pd.DataFrame(columns=cols)\n",
    "best_params = {}\n",
    "best_e = 0\n",
    "best_s = 0\n",
    "\n",
    "for p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12 in zip(hp[n_hp[0]], hp[n_hp[1]], hp[n_hp[2]], hp[n_hp[3]], hp[n_hp[4]], hp[n_hp[5]], hp[n_hp[6]], hp[n_hp[7]], hp[n_hp[8]], hp[n_hp[9]], hp[n_hp[10]], hp[n_hp[11]]): \n",
    "    print('Testing with hyperparameters: ', p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12)\n",
    "    for conv_id, subreddit in val:\n",
    "        conversation, speaker, listener = helpers.extract_responses(conv_id, subreddit, df)\n",
    "\n",
    "        # Predict engagement\n",
    "        num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "        engagement_score = p2*num_turns + p3*interleaved + p4*token_length_score + p5*num_turn_diff\n",
    "        engagement = 1 if engagement_score >= p1 else 0\n",
    "        conversation['predicted_engagement'] = engagement\n",
    "\n",
    "        # Predict satisfaction\n",
    "        slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "        satisfaction_score = p7*slope + p8*sentiment_change + p9*grateful_bonus + p10*profanity_penalty + p11*sarcasm_penalty + p12*disagreement_penalty\n",
    "        satisfaction = 1 if satisfaction_score >= p6 else 0\n",
    "        conversation['predicted_satisfaction'] = satisfaction\n",
    "\n",
    "        df_val_preds = df_val_preds.append(conversation)\n",
    "\n",
    "    df_val_preds = df_val_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', 'predicted_satisfaction',\\\n",
    "                                 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "    \n",
    "    first_utters_val = df_val_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "\n",
    "    P_s, R_s, f1_s, acc_s, P_e, R_e, f1_e, acc_e = helpers.test(first_utters_val)\n",
    "    \n",
    "    if f1_e > best_e:\n",
    "        best_e = f1_e\n",
    "        best_params[n_hp[0]] = p1\n",
    "        best_params[n_hp[1]] = p2\n",
    "        best_params[n_hp[2]] = p3\n",
    "        best_params[n_hp[3]] = p4\n",
    "        best_params[n_hp[4]] = p5\n",
    "        best_params['P_e'] = P_e\n",
    "        best_params['R_e'] = R_e\n",
    "        best_params['f1_e'] = f1_e\n",
    "        best_params['acc_e'] = acc_e\n",
    "        print('Current best hyperparameters are :', best_params)\n",
    "        \n",
    "    if f1_s > best_s:\n",
    "        best_s = f1_s\n",
    "        best_params[n_hp[5]] = p6\n",
    "        best_params[n_hp[6]] = p7\n",
    "        best_params[n_hp[7]] = p8\n",
    "        best_params[n_hp[8]] = p9\n",
    "        best_params[n_hp[9]] = p10\n",
    "        best_params[n_hp[10]] = p11\n",
    "        best_params[n_hp[11]] = p12\n",
    "        best_params['P_s'] = P_s\n",
    "        best_params['R_s'] = R_s\n",
    "        best_params['f1_s'] = f1_s\n",
    "        best_params['acc_s'] = acc_s\n",
    "        print('Current best hyperparameters are :', best_params)\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# TODO: Give best hyperparameters found in validation\n",
    "# TODO: Predict for the test set:\n",
    "for conv_id, subreddit in test:\n",
    "    conversation, speaker, listener = extract_responses(conv_id, subreddit)\n",
    "    \n",
    "    num_turns, interleaved, token_length_score, num_turn_diff, conversation = engagement_preprocessing(speaker, listener, conversation)\n",
    "    engagement_score = num_turns_weight*num_turns + interleaved_weight*interleaved + token_length_weight*token_length_score + diff_weight*num_turn_diff\n",
    "    engagement = 1 if engagement_score >= eng_threshold else 0\n",
    "    conversation['predicted_engagement'] = engagement\n",
    "    \n",
    "    slope, sentiment_change, grateful_bonus, profanity_penalty, sarcasm_penalty, disagreement_penalty = satisfaction_preprocessing(conversation, speaker, tokenizer, model)\n",
    "    satisfaction_score = slope_weight*slope + sentiment_change_weight*sentiment_change + grateful_bonus_weight*grateful_bonus + profanity_penalty_weight*profanity_penalty + sarcasm_penalty_weight*sarcasm_penalty + disagreement_penalty_weight*disagreement_penalty\n",
    "    satisfaction = 1 if satisfaction_score >= sat_threshold else 0\n",
    "    conversation['predicted_satisfaction'] = satisfaction\n",
    "    \n",
    "    df_test_preds = df_test_preds.append(conversation)\n",
    "\n",
    "df_test_preds = df_test_preds[['conversation_id', 'subreddit', 'post_title', 'author', 'dialog_turn', 'text', 'ground_truth_satisfaction', 'ground_truth_engagement', \n",
    "                'predicted_satisfaction', 'predicted_engagement', 'compound', 'sentiment', 'emotion_prediction', 'token_length', 'sentences', 'sentence_compounds', 'strongest_compound']]\n",
    "df_test_preds.to_csv(\"data/RED/annotated/test_predictions.csv\", index=False)\n",
    "\n",
    "df_test_preds = df_test_preds.groupby(['conversation_id', 'subreddit']).first().reset_index()\n",
    "# TODO: Test performance of test set:\n",
    "test_P_s, test_R_s, test_f1_s, test_acc_s, test_P_e, test_R_e, test_f1_e, test_acc_e = helpers.test(first_utters_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEPS: \n",
    "1. Apply algorithm with best values to entire dyadic dataset\n",
    "2. Retrain emobert on whole dataset on sentence level\n",
    "3. Write guidelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
